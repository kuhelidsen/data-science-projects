---
title: "'R'Project_Best_Model_for_Predicting_InsuranceCharges"
author: "Kuheli Sen"
date: "8/11/2020"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Step 1. Data Preparation

## a. Load the dataset insurance.csv Preview the document into memory.

### Answer 1a.
I loaded the dataset into R memory and changed it to a data frame. It has 
1338 rows and 7 columns. I changed the dataset into a data frame.

```{r, include=TRUE}
# I loaded the dataset into R memory as follows:
library(readr)
insurance <- read_csv("./insurance.csv")

# I printed the dataset as follows:
print(insurance)

# Dimensions of the dataset:
dim(insurance)

# I changed the dataset into dataframe as follows:
insurance <- as.data.frame(insurance)
```

## b. In the data frame, transform the variable charges by setting insurance$charges =         log(insurance$charges). Do not transform it outside of the data frame.

### Answer 1b. 
 I took log value of the variable charges and created a new variable "charges_log"

**I used "charges_log" as response for all the Questions** 

 I also changed the catagorical variables "sex", "smoker", "region" into factor.
 
```{r, include=TRUE}
# I transformed the variable charges as follows:
insurance$charges_log = log(insurance$charges)

# I transformed the following catagorical variables into factor as follows:
insurance$sex = factor(insurance$sex)
insurance$smoker = factor(insurance$smoker)
insurance$region = factor(insurance$region)
```

## c. Using the data set from 1.b, use the model.matrix() function to create another data set that uses dummy variables in place of categorical variables. Verify that the first column only has ones (1) as values, and then discard the column only after verifying it has only ones as values.

### Answer 1c. 
I created another data set 'mm' that uses dummy variables in place
of categorical variables and then removed the intercept column. 


```{r, include=TRUE}
# I used model.matrix() to create another dataset with dummy variables:
mm = as.data.frame(model.matrix(
  ~ ., data = insurance))

# I discarded the first column "intercept" as follows:
mm = as.data.frame(model.matrix(
  ~ ., data = insurance))[ ,-1]

# I printed the dataset to verify that the first column "intercept" is removed:
print(head(mm))
```

## d. Use the sample() function with set.seed equal to 1 to generate row indexes for your training and tests sets, with 2/3 of the row indexes for your training set and 1/3 for your test set. Do not use any method other than the sample() function for splitting your data.

### Answer 1d. 
I have generated row indexes with 2/3 of the rows for training set and 1/3 of the rows for the test set.

```{r, include=TRUE}
# I have used set.seed() for reproducible results:
set.seed(1)

# I used sample() to generate row indexes as follows:
index <- sample(1:nrow(insurance),2*nrow(insurance)/3)
```

## e. Create a training and test data set from the data set created in 1.b using the training and test row indexes created in 1.d. Unless otherwise stated, only use the training and test data sets created in this step.

### Answer 1e.
I have created a training and test dataset from dataset created in 1b 
using the training and test row indexes created in 1.d.

```{r, include=TRUE}
# I have created the training and test datasets from dataset created in 1b as follows:
train = insurance[index, ]

test  = insurance[-index, ]
```

## f. Create a training and test data set from data set created in 1.c using the training and test row indexes created in 1.d

### Answer 1f. 
I created another training and test dataset from data set created in 1.c 

```{r, include=TRUE}
# I have created training and test data set from data set created in 1.c as follows:
train_mm = mm[index, ]

test_mm  = mm[-index, ]
```

## Step 2. Build a multiple linear regression model.

## a. Perform multiple linear regression with charges as the response and the predictors are age, sex, bmi, children, smoker, and region. Print out the results using the summary() function. Use the training data set created in step 1.e to train your model.

### Answer 2a. 
I performed multiple linear regression and summarized the results as:
            Multiple R-squared:  0.7839
            p-value: < 2.2e-16 

```{r, include=TRUE} 
# I performed the multiple linear regression as follows:
lm.fit = lm(charges_log ~ age + sex + bmi + children + smoker + region, data=train)

# I used summary() to print out the results:
summary(lm.fit)
```

## b. Is there a relationship between the predictors and the response?

### Answer 2b.
Based on p-value (< 2.2e-16), there is a statistically significant relationship between the predictors and the response as p-value is less than 5%. The relationship is not very strong 
as Multiple R-squared (0.7839) is quite smaller than 1. 

## c. Does sex have a statistically significant relationship to the response?

### Answer 2c. 
Yes, sex has a statistically significant relationship to the response as p-value 
is less than 5% ( p-value: 0.027847).

## d. Perform best subset selection using the stepAIC() function from the MASS library, choose best model based on AIC. For the "direction" parameter in the stepAIC() method, set direciton="backward"

### Answer 2d. 
The best model has the following predictors: 

"age", "sex", "bmi", "children", "smoker", "region"

AIC: 1019.4

```{r, include=TRUE}
# I performed the  regression as follows:
lm.fit = glm(charges_log ~ age + sex + bmi + children + smoker + region, data=train)

# I loaded the library MASS as follows:
library(MASS)

# I performed best subset selection using the stepAIC() as follows:

fit.best <- stepAIC(lm.fit, scope=list(lower= ~ 1), direction="backward")

# I printed out the model results as follows:
summary(fit.best)
```

## e. Compute the test error of the best model in #2d based on AIC using LOOCV using trainControl() and train() from the caret library. Report the MSE by squaring the reported RMSE.

### Answer 2e.
The test error (MSE) of the best model in 2d is as follows:
           MSE:  (0.4458979)^2 = 0.1988249
           
```{r, include=TRUE} 
## I loaded the library into memory:
library(caret)

## I defined training control by specifying LOOCV:
train_control_2e = trainControl(method = "LOOCV")

## I trained the model as folows:
model_2e <- train(charges_log ~ children + bmi + region + sex + age + smoker, trControl = train_control_2e, data= insurance,  method = "lm")

## I printed model as follows: 
print(model_2e)
```

## f. Calculate the test error of the best model in #2d based on AIC using 10-fold Cross-Validation. Use train and trainControl from the caret library. Refer to model selected in #2d based on AIC. Report the MSE.

### Answer 2f. 
The test error (MSE) of the best model in 2d is as follows: 
             MSE: (0.4449095)^2 = 0.1979445

```{r, include=TRUE}
# for reproducibility:
set.seed(1)

## I loaded the library into memory:
library(caret)

## I defined training control by specifying CV:
train_control_2f = trainControl(method = "CV", number = 10)

## I trained the model as folows:
model_2f <- train(charges_log ~ children + bmi + region + sex + age + smoker, trControl = train_control_2f, data= insurance,  method = "lm")

## I printed model as follows: 
print(model_2f)
```

## g. Calculate and report the test MSE using the best model from 2.d and test data set created in step 1.e.

### Answer 2g. 
MSE:  0.231291

```{r, include=TRUE}
# I predicted the test data as follows:
pred = predict(fit.best, newdata = test )

# I computed the mean squared error as follows:
MSE_2g = mean((pred - test$charges_log)^2)

# I printed out the test test error (MSE) as follows:
MSE_2g
```

## h. Compare the test MSE calculated in step 2.f using 10-fold cross-validation with the test MSE calculated in step 2.g. How similar are they?

### Answer 2h.
(2f)
 10-fold CV
 MSE:  0.1979445

 (2g)
 test error
 MSE:  0.231291

**The test MSE calculated in step 2f using 10-fold cross-validation (0.1979445) is less than the test MSE calculated in step 2g (0.231291). 10- fold cross validation is more accurate way to calculate the test error as compared to 2g**
 
## Step 3.  Build a regression tree model.

## a. Build a regression tree model using function tree(), where charges is the response and the predictors are age, sex, bmi, children, smoker, and region.

### Answer 3a. 
Variables actually used in tree construction:
         "bmi" "children" "age"  "smoker"

```{r, include=TRUE}  
# I installed the package tree into R and loaded the library as follows:
library(tree)

# I have built a tree model using tree() and summarized it as follows:
tree.insurance = tree( charges_log ~ age + sex + bmi + children + smoker + region, insurance, subset = index)
summary(tree.insurance)
```

## b. Find the optimal tree by using cross-validation and display the results in a graphic. Report the best size.

### Answer 3b.
I performed the cross validation and plotted the results with tree size on X-axis and test error on the Y-axis. 
         Best tree size: 4
         
```{r, include=TRUE}  
# I plotted the tree by using plot() and then used text() to label the tree nodes as follows:
plot(tree.insurance)
text(tree.insurance, pretty = 0)

# I performed cv() to choose the best model as follows:
cv.insurance = cv.tree(tree.insurance)

# I ploted the cross validation results to choose the best tree size as follows:
# x axis is size of the tree model and y axis is the test error:
plot(cv.insurance$size, cv.insurance$dev, ttpe = 'b')
```

## c. Justify  the number you picked for the optimal tree with regard to the principle of variance-bias trade-off.

### Answer 3c. 
The tree size 7 and 8 have the smallest test error. These models are overfitting (high variance) and are very complex models so I donot want to choose these models. Tree size 4 has smaller test error as compared to the more simpler models (1, 2, 3) which are underfitting (high bias). This model (size 4) is much simpler than models 7 and 8. 

**I would choose tree size 4 as the best model with an optimal balance of bias and variance.Tree model size 4 is simple and easy to explain to the management.**

## d. Prune the tree using the optinal size found in 3.b

### Answer 3d.
I pruned the tree by using the best size "4" and then plotted the pruned tree.

```{r, include=TRUE}
# I pruned the tree with the best size "4" as follows:
prune.insurance = prune.tree(tree.insurance, best = 4)
```

## e. Plot the best tree model and give labels.

### Answer 3e.
```{r, include=TRUE}
# Ploted the best tree model with labels:
plot(prune.insurance)
text(prune.insurance, pretty = 0)
```

## f. Calculate the test MSE for the best model.

### Answer 3f.
MSE: 0.2421874

```{r, include=TRUE}
# I used predict() to forecast the test dataset as follows:
yhat = predict(prune.insurance, newdata = insurance[-index, ])

# Actual observation in the test dataset:
insurance.test = insurance[-index, "charges_log"]

# I computed the MSE(test error) as follows:
MSE_3f = mean((yhat - insurance.test)^2)
MSE_3f 
```

## Step 4. Build a random forest model.

## a. Build a random forest model using function randomForest(), where charges is the response and the predictors are age, sex, bmi, children, smoker, and region.

### Answer 4a.
I used randomForest() to build a random forest model.

```{r, include=TRUE}
# I loaded the library into R as follows:
library(randomForest)

# I have used randomForest() to build the model as follows:
rf.insurance = randomForest(charges_log ~ age + sex + bmi + children + smoker + region, data = insurance, subset = index, importance = TRUE)
```

## b. Compute the test error using the test data set.

### Answer 4b.
To compute test error, first of all I predicted the test data using predict() and then 
calculated test error of the test dataset by finding the mean squared difference between predicted data and actual data.

 MSE (test error): 0.1786004

```{r, include=TRUE}
# I used predict() to forecast the test dataset as follows:
yhat.rf = predict(rf.insurance, newdata = insurance[-index, ])

# Actual observation in the test dataset:
insurance.test = insurance[-index, "charges_log"]

# I computed the MSE(test error) as follows:
MSE_4b = mean((yhat.rf - insurance.test)^2)
MSE_4b
```

## c. Extract variable importance measure using the importance() function.

### Answer 4c.
I extracted the variable importance and got the following results. The %IncMSE represents
the importance of each variable. Higher values represent higher importance.

```{r, include=TRUE}
# I extracted the variable importance as follows:
importance(rf.insurance)
```

## d. Plot the variable importance using the function, varImpPlot(). Which are the top 3 important predictors in this model?

### Answer 4d.
I plotted the variable importance using function varImpPlot(). 

 The top 3 important predictors in this model are as follows:

 **"smoker", "age", "children"**
 
```{r, include=TRUE}
# I plotted the variable importance as follows:
varImpPlot(rf.insurance)
```

## Step 5. Build a support vector machine model

## a. The response is charges and the predictors are age, sex, bmi, children, smoker, and region. Please use the svm() function with radial kernel and gamma=5 and cost = 50.

### Answer 5a.
I used svm() to build the model on training dataset with the parameters mentioned above,
then I printed out the results using summary(). The model results are as follows:

Number of Support Vectors:  715

```{r, include=TRUE}
## I have loaded the library e1071 as follows:
library(e1071)

# I have build up support vector machine and summarized it as follows::
svm.fit = svm( charges_log ~ age + sex + bmi + children + smoker + region, data = insurance[index, ], kernel = "radial", gamma = 5, cost = 50)
summary(svm.fit)
```

## b. Perform a grid search to find the best model with potential cost: 1, 10, 50, 100 and potential gamma: 1,3 and 5 and potential kernel: "linear", "polynomial", "radial" and "sigmoid". And use the training set created in step 1.e.

### Answer 5b.
I performed a grid search using tune() to find out the best model parameters
with the above mentioned potential kernel, cost and gamma values on training dataset.
tune() function will search (4 x 3) 12 potential combinations.

```{r, include=TRUE}
# I performed a grid search to find the best model as follows:
tune.out = tune( svm, charges_log ~ age + sex + bmi + children + smoker + region, data = insurance[index, ], kernel = c("linear","polynomial","radial","sigmoid"),  ranges = list( cost = c(1, 10, 50, 100), gamma = c(1,3,5)))
```

## c. Print out the model results. What are the best model parameters?

### Answer 5c.
best parameters:
   cost gamma
    1     1

best performance: 0.194239  
          
```{r, include=TRUE}  
# The model results are as follows:
summary(tune.out)
```

## d. Forecast charges using the test dataset and the best model found in c).

### Answer 5d.
I used predict() to forecast "charges_log" using the test dataset and the best model found in (c).

```{r, include=TRUE}
# I predicted the best model as follows:
pred = predict(tune.out$best.model, newdata = insurance[-index, ])

# I got the true observation of charges of the test dataset as follows: 
trueObservation  = insurance[-index, "charges_log" ]
```

## e. Compute the MSE (Mean Squared Error) on the test data.

### Answer 5e.
MSE: 0.2567148

```{r, include=TRUE}
# I computed the MSE as follows:
mean((pred - trueObservation)^2)
```

## Step 6. Perform the k-means cluster analysis.

## a. Use the training data set created in step 1.f and standardize the inputs using the scale() function.

### Answer 6a. 
I standardized the inputs using scale() so that each input has equal weightage 
for k-means cluster analysis.

```{r, include=TRUE}
# I normalized the data as follows:
insurance_scaled = scale(train_mm)
```

## b. Convert the standardized inputs to a data frame using the as.data.frame() function.

### Answer 6b. 
I converted the scaled data into a data frame.

```{r, include=TRUE}
# I changed the normalized data into a dataframe as follows:
insurance_scaled = as.data.frame(insurance_scaled)
```

## c. Determine the optimal number of clusters, and use the gap_stat method and set iter.max=20. Justify your answer.

### Answer 6c. 
The value for optimal number of clusters is "2". I used fviz_nbclust() to find optimal number of clusters.The ideal number of clusters is chosen based on the Euclidean distance and the variation within a cluster. The number of clusters can be chosen as the smallest value of k such that the gap statistic is within one standard deviation of the gap at k+1.

```{r, include=TRUE}
# I loaded the libraries as follows:
library("cluster")
library("factoextra")

# I used the following function to determine the optimal number of clusters:
fviz_nbclust(insurance_scaled, kmeans, method = "gap_stat", iter.max=20)
```

## d. Perform k-means clustering using the optimal number of clusters found in step 6.c. Set parameter nstart = 25

### Answer 6d. 
I performed k-means clustering using optimal number of clusters : 2

```{r, include= TRUE}
# I performed k-means clustering as follows:
km.res = kmeans(insurance_scaled, 2, nstart = 25)
```

## e. Visualize the clusters in different colors, setting parameter geom="point"

### Answer 6e.
I used fviz_cluster() to visualize the clusters in two different colors.

```{r, include=TRUE}
# I visualized the clusters as follows:
fviz_cluster(km.res, data = insurance_scaled, geom = "point")
```

## Step 7. Build a neural networks model.

## a. Using the training data set created in step 1.f, create a neural network model where the response is charges and the predictors are age, sexmale, bmi, children, smokeryes, regionnorthwest, regionsoutheast, and regionsouthwest. Please use 1 hidden layer with 1 neuron. Do not scale the data.

### Answer 7a.
I used the training dataset "train_mm" to create a neural network model.

```{r, include=TRUE}
# I loaded the library as follows:
library(neuralnet)

# I  created a neural network model as follows:
nn.model <- neuralnet( charges_log ~ age + sexmale + bmi + children + smokeryes + regionnorthwest + regionsoutheast + regionsouthwest, data = train_mm, hidden = c(1))
```

## b. Plot the neural network.

### Answer 7b. 
I plotted the neural network which shows 1 hidden layer with 1 neuron. 

```{r, include=TRUE}
# I plotted the model as follows:
plot(nn.model)
```

## c. Forecast the charges in the test dataset.

### Answer 7c.
I predicted the net results for "charges_log" as predict.nn

```{r, include=TRUE}
# I predicted the results as follows:
predict.nn = compute(nn.model, test_mm[, c("age", "sexmale", "bmi", "children", "smokeryes", "regionnorthwest", "regionsoutheast", "regionsouthwest")])
```

## d. Compute test error (MSE).

### Answer 7d.
MSE (test error) : 0.8737048

```{r, include=TRUE}
# I got the true observation of test data as follows:
observe.test = test_mm$charges_log

# I calculated the MSE as follows:
MSE_7d = mean((observe.test - predict.nn$net.result)^2)
MSE_7d 
```

## Step 8. Putting it all together.

## a. For predicting insurance charges, your supervisor asks you to choose the best model among the multiple regression, regression tree, random forest, support vector machine, and neural network models. Compare the test MSEs of the models  generated in steps 2.g, 3.f, 4.b, 5.e, and 7.d. Display the names  for these types of these models, using these labels: "Multiple Linear Regression", "Regression Tree", "Random Forest",  "Support Vector Machine", and "Neural Network" and their  corresponding test MSEs in a data.frame. Label the column in your  data frame with the labels as "Model.Type", and label the column  with the test MSEs as "Test.MSE" and round the data in this column to 4 decimal places. Present the formatted data to your supervisor and recommend which model is best and why.

### Answer 8a.
 Random Forest model has the least MSE so it is supposed to be the best model.
 Random Forest model has smaller variance and better prediction power.
 But it is a very complex model and difficult to explain to the management. The
 Regression Tree model also has a low test error, it is much easier to explain 
 tree models  than the linear regression models.Trees can be displayed graphically 
 and are easily interpreted even by a non-expert. 
         
**I would recommend "Random Forest model" as the best model for high accuracy and low variance, but if I have to present the data and explain it to the management I would suggest "Regression Tree Model" as the best model to my supervisor.**

```{r, include=TRUE}
# I created a data frame to compare the test MSEs of different models as follows:
df = data.frame(Model.Type = c("Multiple Linear Regression", "Regression Tree", 
                               "Random Forest", "Support Vector Machine", "Neural Network"),
                          Test.MSE = c(0.2313, 0.2422, 0.1786, 0.2567, 0.8737))

# I printed df as follows:
df
```

## b. Another supervisor from the sales department has requested your help to create a predictive model that his sales  representatives can use to explain to clients what the potential costs could be for different kinds of customers, and they need an easy and visual way of explaining it. What model would  you recommend, and what are the benefits and disadvantages of your recommended model compared to other models?

### Answer 8b. 
I would recommend "Regression Tree model" as it has a lower MSE as compared to other models and it is simpler than "random forest model" which has lowest MSE but it is very complicated and difficult to explain. The following are the advantages and disadvantages of Regression Tree Model.

**Advantages of using Regression Tree Model**
  1. It is much easier to explain Regression tree models to people as compared to other models
  2. Regression trees can be displayed graphically and are easily interpreted even  by a            non-expert.
  3. Trees can easily handle categorical predictors.
         
**Disadvantages of tree model:**
  1. Tree models donot have a high level of accuracy as random forest models.
  2. Tree models could be non robust and they may have large variance. 
  
## c. The supervisor from the sales department likes your regression tree model. But she says that the sales people say the numbers in it are way too low and suggests that maybe the numbers on the leaf nodes predicting charges are log transformations of the actual charges. You realize that in step 1.b of this project that you had indeed transformed charges using the log function. And now you realize that you need to reverse the transformation in your final output. The solution you have is to reverse the log transformation of the variables in  the regression tree model you created and redisplay the result. Follow these steps:

##  i.   Copy your pruned tree model to a new variable.

### Answer 8c i. 
I created copy of pruned tree model as "copy_of_my_pruned_tree".

```{r, include=TRUE}
# I created a copy as follows:
copy_of_my_pruned_tree <- prune.insurance
```

## ii. In your new variable, find the data.frame named "frame" and reverse the log transformation on the data.frame column yval using the exp() function. (If the copy of your pruned tree model is named  copy_of_my_pruned_tree, then the data frame is  accessed as copy_of_my_pruned_tree$frame, and it works just like a normal data frame.).

### Answer 8c ii. 
I reversed the log transformation on the data.frame column yval using the exp() function.

```{r, include = TRUE}
# I have printed out the copy as follows:
copy_of_my_pruned_tree

# I performed exp() for yval as follows:
copy_of_my_pruned_tree$frame$yval <- exp(copy_of_my_pruned_tree$frame$yval)
```

## iii. After you reverse the log transform on the yval column, then replot the tree with labels.

### Answer 8c iii. 
I replotted the tree with labels after reversing the log transformation. 

```{r, include=TRUE}
# I plotted the tree after I reversed the log transform on the yval column as follows:
 plot(copy_of_my_pruned_tree)
 text(copy_of_my_pruned_tree, pretty = 0)
```


  




